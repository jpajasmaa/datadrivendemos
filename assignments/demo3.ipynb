{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357964b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbe4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c850693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from desdeo_emo.EAs import RVEA, NSGAIII\n",
    "from desdeo_problem.testproblems.TestProblems import test_problem_builder\n",
    "from desdeo_problem import DataProblem\n",
    "from desdeo_tools.utilities import fast_non_dominated_sort, hypervolume_indicator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from pyDOE import lhs\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared, WhiteKernel, RationalQuadratic, DotProduct, ConstantKernel, Matern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aef005",
   "metadata": {},
   "source": [
    "## Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f09ff5",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "\n",
    " Use EI and the mean prediction to solve any single objective benchmark problem (e.g.\n",
    "Ackley, Rosenblock, sphere etc.) with any single objective optimizer (preferably GA). \n",
    "\n",
    "Set max exact function evaluations to 50 (start with 50 design points). Was the solutions\n",
    "found by EI better? (you can implement EI is you wish to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84eb57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(dec_dim, samples, bounds):\n",
    "    # create samples \n",
    "    x = lhs(dec_dim, samples)    \n",
    "    # scale\n",
    "    #lower = bounds[0]\n",
    "    #upper = bounds[1]\n",
    "    #x = x * (upper - lower) + lower    \n",
    "    return np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06dc5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem is the expensive function to evaluate.\n",
    "# use ackley\n",
    "\n",
    "\n",
    "# prob takes now all as vectors, need to change GA to do so too.\n",
    "def problem(x):\n",
    "    # if only one solution to calculate\n",
    "    if x.shape[0] == 2:\n",
    "        term1 = -20 * np.exp(-0.2 * np.sqrt(0.5 * (x[0]**2 + x[1]**2)))\n",
    "        term2 = np.exp(0.5 * (np.cos(2 * np.pi * x[0]) + np.cos(2 * np.pi * x[1])))\n",
    "    else:\n",
    "        term1 = -20 * np.exp(-0.2 * np.sqrt(0.5 * (x[:,0]**2 + x[:,1]**2)))\n",
    "        term2 = np.exp(0.5 * (np.cos(2 * np.pi * x[:,0]) + np.cos(2 * np.pi * x[:,1])))\n",
    "    return term1 - term2 + np.exp(1) + 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf80f62",
   "metadata": {},
   "source": [
    "### Using mean pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b62ae39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "best y 1.1582425389399553\n",
      "best x [0.17630633 0.01764189]\n",
      "==============================\n",
      "best y 2.410305440792584\n",
      "best x [0.21093992 0.23536211]\n",
      "==============================\n",
      "best y 0.2524720165472729\n",
      "best x [0.01499056 0.05623917]\n",
      "==============================\n",
      "best y 1.06739562972189\n",
      "best x [0.16657781 0.00579117]\n",
      "==============================\n",
      "best y 0.3899724330096994\n",
      "best x [0.01794127 0.07821266]\n",
      "==============================\n",
      "best y 0.4168110473219393\n",
      "best x [0.03882171 0.07456865]\n",
      "==============================\n",
      "best y 0.9682534720941227\n",
      "best x [0.10376224 0.11220651]\n",
      "==============================\n",
      "best y 0.5155728366532379\n",
      "best x [0.06277627 0.07494959]\n",
      "==============================\n",
      "best y 0.5004957651371846\n",
      "best x [0.03912415 0.08757061]\n",
      "==============================\n",
      "best y 0.28169109905709533\n",
      "best x [0.03504639 0.05247884]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "def surrogate(model, x):\n",
    "    return model.predict(x, return_std=True)\n",
    "\n",
    "def posteori_mean_pred(x, model):\n",
    "    return model.predict(x)\n",
    "    \n",
    "def opt_acq(x, y, model):\n",
    "    x_samples = create_samples(2, 50, bounds)\n",
    "    \n",
    "    scores = posteori_mean_pred(x_, model)\n",
    "    best_x = np.argmin(scores)\n",
    "    \n",
    "    sampled_scores = posteori_mean_pred(x_samples, model)\n",
    "    best_sampled = np.argmin(sampled_scores)\n",
    "    \n",
    "    if x[best_x] < x_samples[best_sampled]:\n",
    "        return best_x\n",
    "    else:\n",
    "        return best_sampled\n",
    "\n",
    "mean_pred_result_all = []\n",
    "times = 10\n",
    "bounds = np.array([[-2, -2], [2, 2]])\n",
    "\n",
    "for _ in range(times):\n",
    "       \n",
    "    x = create_samples(2, 50, bounds)\n",
    "    y = problem(x)\n",
    "    kernel = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\n",
    "    model = GaussianProcessRegressor(kernel,n_restarts_optimizer=1,random_state=7).fit(x, y)\n",
    "    \n",
    "    fmax = 0           \n",
    "    while fmax < 50:\n",
    "        best_x = np.argmin(posteori_mean_pred(x, model))\n",
    "        best_x = x[best_x]\n",
    "        \n",
    "        true_y = problem(best_x) # + 1 fmax..\n",
    "        fmax += 1\n",
    "    \n",
    "        x = np.vstack((x, [best_x]))\n",
    "        y = np.hstack((y, [true_y]))\n",
    "\n",
    "        # update surr\n",
    "        model.fit(x, y)\n",
    "            \n",
    "  \n",
    "    print(\"==============================\")\n",
    "    fit = np.min(y)\n",
    "    print(\"best y\", fit)\n",
    "    print(\"best x\", x[np.argmin(y)])\n",
    "\n",
    "    mean_pred_result_all.append(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9686bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1582425389399553,\n",
       " 2.410305440792584,\n",
       " 0.2524720165472729,\n",
       " 1.06739562972189,\n",
       " 0.3899724330096994,\n",
       " 0.4168110473219393,\n",
       " 0.9682534720941227,\n",
       " 0.5155728366532379,\n",
       " 0.5004957651371846,\n",
       " 0.28169109905709533]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pred_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf0bd7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7961212279274982"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_pred_result_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df40e9",
   "metadata": {},
   "source": [
    "### single opt GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dedc61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class real_GA:\n",
    "    def __init__(self, problem, pop, pop_size, pm, bounds, di, order, fitness=None, max_func_evals=50, gen_max=50):\n",
    "        self.problem = problem # problem function to solve\n",
    "        self.pop = pop # pop array to hold binary population strings\n",
    "        self.pm = pm # probability of mutation\n",
    "        self.pop_size = pop_size # population size\n",
    "        self.lbounds = bounds[0]\n",
    "        self.ubounds = bounds[1]\n",
    "        self.di = di # for crossover\n",
    "        self.order = order # for mutation\n",
    "        self.gen_max = gen_max # max generations\n",
    "        self.max_func_evals = max_func_evals\n",
    "        \n",
    "        self.fitness = fitness # fitness array to hold calculated fitness values\n",
    "        self.gen = 0 # current generation\n",
    "        \n",
    "    # start pop and evaluate each member in the pop\n",
    "    def initialize(self):\n",
    "        if len(self.pop) < 1:\n",
    "            x1range = np.random.uniform(low=self.lbounds[0], high=self.ubounds[0], size=self.pop_size)\n",
    "            x2range = np.random.uniform(low=self.lbounds[1], high=self.ubounds[1], size=self.pop_size)\n",
    "            self.pop = np.stack((x1range, x2range), axis=-1)\n",
    "\n",
    "        #self.pop_size = self.pop.shape[0]\n",
    "        self.fitness = self.evaluate(x)\n",
    "\n",
    "\n",
    "    # run the GA for one iteration\n",
    "    def run(self):\n",
    "        n = 0 # init iterations\n",
    "        self.initialize()\n",
    "\n",
    "        next_gen = [] # init next_gen population array\n",
    "        for i in range(int(self.pop.shape[0]/2)):\n",
    "            # select two individuals with deterministic tournament selection, append them in a next gen list for crossover\n",
    "            i1, i2 = self.tour_select()\n",
    "            s1 = self.pop[i1]\n",
    "            s2 = self.pop[i2]\n",
    "            next_gen.append(s1)\n",
    "            next_gen.append(s2)\n",
    "            \n",
    "        # crossover. Happens every time\n",
    "        next_gen = self.SBX(next_gen)\n",
    "            \n",
    "        # Mutation. happens if rand < pm for member in pop\n",
    "        for i in range(self.pop_size):\n",
    "            if np.random.rand() < self.pm:\n",
    "                next_gen[i] = self.poly_mutation(next_gen[i], self.order)\n",
    "         \n",
    "        self.pop = np.asarray(next_gen) # add next gen to self pop               \n",
    "        self.fitness = self.evaluate(x)\n",
    "        \n",
    "        n += 1\n",
    "        self.gen += 1     \n",
    "    \n",
    "    # evaluate population members\n",
    "    def evaluate(self, x):\n",
    "        return problem(x)\n",
    "        \n",
    "        \n",
    "    # deterministic binary tournament selection\n",
    "    def tour_select(self):        \n",
    "        cf = self.fitness\n",
    "        b1 = np.argmin(cf) # get best member by fitness\n",
    "        cf = np.delete(cf, b1) # remove it from cf\n",
    "        b2 = np.argmin(cf) # get (2nd) best member by fitness\n",
    "        return b1, b2 # return best and 2nd best members as parents\n",
    "        \n",
    "        \n",
    "    # Simulated binary crossover (non-bounded)\n",
    "    def SBX(self, parents):\n",
    "        parents = np.asarray(parents)\n",
    "        pop_size, num_var = parents.shape\n",
    "        children = np.zeros_like(parents)\n",
    "        for i in range(0, pop_size, num_var):\n",
    "            p1 = (parents[i] + parents[i + 1]) / 2\n",
    "            p2 = (parents[i] - parents[i + 1]) / 2\n",
    "            beta = np.zeros(num_var)\n",
    "            alpha = np.random.rand(num_var)\n",
    "            bx = np.random.randint(0, high=2, size=num_var)\n",
    "            beta[alpha <= 0.5] = (2 * alpha[alpha <= 0.5])**(1 / (self.di + 1))\n",
    "            beta[alpha > 0.5] = (2 - 2 * alpha[alpha > 0.5])**(-1 / (self.di + 1))            \n",
    "            beta = beta * ((-1)**bx)\n",
    "            children[i] = p1 + beta * p2\n",
    "            children[i + 1] = p1 - beta * p2\n",
    "        return children\n",
    "        \n",
    "    \n",
    "    # polynomial mutation for one pop member p\n",
    "    def poly_mutation(self, p, order):\n",
    "        children = np.array([0,0])\n",
    "        for i in range(0, 2):\n",
    "            pL = self.lbounds[i]\n",
    "            pU = self.ubounds[i]\n",
    "            u = np.random.random() # r [0,1]\n",
    "            mp = 0\n",
    "            dl = (2*u)**(1/1+order) - 1\n",
    "            dr = 1 - (2*(1 - u))**(1/1+order) \n",
    "            if u <= 0.5:\n",
    "                mp = p[i] + dl*(p[i] - pL)\n",
    "            else:\n",
    "                mp = p[i] + dr*(pU - p[i])\n",
    "            children[i] = mp\n",
    "        return children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b1b9d",
   "metadata": {},
   "source": [
    "## using EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ede7dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "best y 0.452634979296743\n",
      "best x [0.08549487 0.02591514]\n",
      "==============================\n",
      "best y 0.9679218146447894\n",
      "best x [0.02603835 0.15251075]\n",
      "==============================\n",
      "best y 0.02005199020325321\n",
      "best x [-0.00541927  0.00388961]\n",
      "==============================\n",
      "best y 0.0\n",
      "best x [0. 0.]\n",
      "==============================\n",
      "best y 0.6659817764888416\n",
      "best x [0.11260981 0.0345579 ]\n",
      "==============================\n",
      "best y 0.161870191666857\n",
      "best x [0.04094391 0.00579893]\n",
      "==============================\n",
      "best y 0.10194037976392067\n",
      "best x [0.01274589 0.02544643]\n",
      "==============================\n",
      "best y 0.36237825552528236\n",
      "best x [0.04895394 0.05800767]\n",
      "==============================\n",
      "best y 0.6883575322604791\n",
      "best x [0.01905027 0.11931682]\n",
      "==============================\n",
      "best y 0.4393186513698133\n",
      "best x [0.00313146 0.0874817 ]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "EI_result_all = []\n",
    "\n",
    "def surrogate(model, x):\n",
    "    return model.predict(x, return_std=True)\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.special import ndtr\n",
    "\n",
    "def EI(mean, std, max_val, tradeoff):\n",
    "    imp = (mean - max_val - tradeoff)\n",
    "    z = imp / std\n",
    "    ei = imp * norm.cdf(z) + std * norm.pdf(z)\n",
    "    ei[std == 0.0] = 0.0\n",
    "    return ei\n",
    "    \n",
    "\n",
    "def expected_impr(x0, x_sample, y, model):\n",
    "    mu, sigma = model.predict(x0, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    max_val = np.max(y)\n",
    "    tradeoff = 0.01\n",
    "\n",
    "    return EI(mu, sigma, max_val, tradeoff)\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def propose_location(X_sample, Y_sample, model, bounds):\n",
    "    min_x = np.array([1.,1.]) # atleast return something\n",
    "    \n",
    "    def min_obj(X0):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -expected_impr(X0, X_sample, Y_sample, model)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.   \n",
    "    \n",
    "    # use GA here\n",
    "    # params\n",
    "    pop_s = Y_sample.shape[0]\n",
    "    pm = 0.1\n",
    "    bounds = np.array([[-2, -2], [2, 2]]) # variable bounds (lower, upper)\n",
    "    gen_max = 50\n",
    "    di = 2 # distribution index\n",
    "    order = 20 # polynomial order param\n",
    "\n",
    "    pop = create_samples(2, pop_s, bounds)\n",
    "    ga = real_GA(min_obj, pop, pop_s, pm, bounds, di, order, fmax, gen_max)\n",
    "    for _ in range(10):\n",
    "        ga.run() # run ga for 20 iterations\n",
    "    \n",
    "    best_fit = np.argmin(ga.fitness)\n",
    "    min_x = ga.pop[best_fit]      \n",
    "            \n",
    "    return min_x\n",
    "\n",
    "EI_result_all = []\n",
    "times = 10\n",
    "\n",
    "for _ in range(times):\n",
    "    x = create_samples(2, 50, bounds)\n",
    "    y = problem(x)\n",
    "    kernel = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\n",
    "    \n",
    "    model = GaussianProcessRegressor(kernel,n_restarts_optimizer=1,random_state=7).fit(x, y)\n",
    "    \n",
    "    fmax = 0\n",
    "    while fmax < 50:\n",
    "\n",
    "        best_x = propose_location(x, y, model, bounds)\n",
    "        #print(best_x)\n",
    "        true_y = problem(best_x) # + 1 fmax..\n",
    "        fmax += 1\n",
    "        \n",
    "        # add to data. have to keep x, y == pop.size to not break GA currently.\n",
    "        worst_y = np.argmax(y)\n",
    "        x = np.delete(x, worst_y, axis=0)\n",
    "        y = np.delete(y, worst_y, axis=0)\n",
    "        x = np.vstack((x, best_x))\n",
    "        y = np.hstack((y, true_y))\n",
    "\n",
    "        # update surr\n",
    "        model.fit(x, y)\n",
    "    \n",
    "    print(\"==============================\")\n",
    "    fit = np.min(y)\n",
    "    print(\"best y\", fit)\n",
    "    print(\"best x\", x[np.argmin(y)])\n",
    "\n",
    "    EI_result_all.append(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2d63959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.452634979296743,\n",
       " 0.9679218146447894,\n",
       " 0.02005199020325321,\n",
       " 0.0,\n",
       " 0.6659817764888416,\n",
       " 0.161870191666857,\n",
       " 0.10194037976392067,\n",
       " 0.36237825552528236,\n",
       " 0.6883575322604791,\n",
       " 0.4393186513698133]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EI_result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ac26476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38604555712199795"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(EI_result_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef00784",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "Both algorithms are operating with bounds of [-2, -2]. Both use Matern kernel as it seems to work ok with Ackley. Mean prediction samples 50 new points using LHS each iteration. Genetic algorithm uses non-bounded SBX which effects to the variation not sticking to the bounds. GA also runs 10 generations to optimize the acquisicition function. Increasing generations seemed to make GA miss more, probably due to the non bounded SBX. Also since my GA was simple and hardcoded I had to keep the population size at 50.\n",
    "\n",
    "### Mean pred\n",
    "\n",
    "Using mean prediction with 50 real function evaluations between 10 independent runs we gain mean of 0.796 for fitness value. Ackley test problem has global minimum at [0,0] with 0 value. As expected mean prediction seems to get stuck to local minimas around the \n",
    "\n",
    "### EI\n",
    "\n",
    "Using EI and GA with 50 real function evaluations between 10 independet runs we gain mean of 0.38 for fitness value. It is almost twice better than with mean prediction and EI also has less worse results and most times atleast one independent run from 10 finds the global minimum of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a95d55",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Solve any benchmark problems (K=2 and 5, n=10) with ParEGO and LCB.\n",
    "Start with 109 design points. Compare the hypervolume of the solutions after 100 exact function\n",
    "evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d5b4a",
   "metadata": {},
   "source": [
    "Trying Botorch to solve these. It leads to using UCB instead of LCB since Botorch assumes maximization.\n",
    "\n",
    "https://botorch.org/tutorials/multi_objective_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18acbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60ff53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.test_functions import DTLZ2\n",
    "\n",
    "problem = DTLZ2(dim=10, num_objectives=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b93093ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.ref_point = torch.tensor([0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb531d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.ref_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ce5ee6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70f6fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "NOISE_SE = torch.tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 5 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 64 if not SMOKE_TEST else 4\n",
    "\n",
    "standard_bounds = torch.zeros(5, problem.dim)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "def generate_initial_data(n=109):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(bounds=problem.bounds,n=n, q=1).squeeze(1)\n",
    "    train_obj_true = problem(train_x)\n",
    "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "    return train_x, train_obj, train_obj_true\n",
    "\n",
    "\n",
    "def initialize_model(train_x, train_obj):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i:i+1]\n",
    "        train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            FixedNoiseGP(train_x, train_y, train_yvar, outcome_transform=Standardize(m=1))\n",
    "        )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1903ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from botorch.acquisition.monte_carlo import qUpperConfidenceBound\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "def optimize_qucb_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qucb acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "\n",
    "    weights = sample_simplex(problem.num_objectives).squeeze()\n",
    "    objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=pred))\n",
    "    acq_func = qUpperConfidenceBound(\n",
    "        model=model,\n",
    "        beta=0.5,\n",
    "        sampler=sampler,\n",
    "        objective=objective,\n",
    "        #posteori_transform=,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 30},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    \n",
    "    new_obj = new_obj_true #+ torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abefcc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "\n",
    "\n",
    "def optimize_qnparego_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization \n",
    "    of the qNParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x).mean\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.num_objectives).squeeze()\n",
    "        objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=pred))\n",
    "        acq_func = qNoisyExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            X_baseline=train_x,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 30},\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    \n",
    "    # Here happens function evaluation\n",
    "    new_obj_true = problem(new_x)\n",
    "    \n",
    "    \n",
    "    new_obj = new_obj_true # + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66e418fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  1: Hypervolume (qNParEGO, qUCB) = (3.53, 3.53), time = 3.47.\n",
      "Batch  2: Hypervolume (qNParEGO, qUCB) = (3.53, 3.53), time = 3.18.\n",
      "Batch  3: Hypervolume (qNParEGO, qUCB) = (3.53, 3.53), time = 2.32.\n",
      "Batch  4: Hypervolume (qNParEGO, qUCB) = (3.53, 3.62), time = 2.67.\n",
      "Batch  5: Hypervolume (qNParEGO, qUCB) = (3.53, 3.62), time = 2.54.\n",
      "Batch  6: Hypervolume (qNParEGO, qUCB) = (3.53, 3.62), time = 2.85.\n",
      "Batch  7: Hypervolume (qNParEGO, qUCB) = (3.53, 3.62), time = 2.65.\n",
      "Batch  8: Hypervolume (qNParEGO, qUCB) = (3.53, 3.63), time = 3.00.\n",
      "Batch  9: Hypervolume (qNParEGO, qUCB) = (3.53, 3.63), time = 3.49.\n",
      "Batch 10: Hypervolume (qNParEGO, qUCB) = (3.53, 3.63), time = 2.70.\n",
      "Batch 11: Hypervolume (qNParEGO, qUCB) = (3.53, 3.63), time = 3.26.\n",
      "Batch 12: Hypervolume (qNParEGO, qUCB) = (3.53, 3.63), time = 3.16.\n",
      "Batch 13: Hypervolume (qNParEGO, qUCB) = (3.53, 3.66), time = 4.45.\n",
      "Batch 14: Hypervolume (qNParEGO, qUCB) = (3.54, 3.66), time = 4.25.\n",
      "Batch 15: Hypervolume (qNParEGO, qUCB) = (3.69, 3.67), time = 3.90.\n",
      "Batch 16: Hypervolume (qNParEGO, qUCB) = (3.79, 3.67), time = 2.57.\n",
      "Batch 17: Hypervolume (qNParEGO, qUCB) = (3.79, 3.67), time = 4.36.\n",
      "Batch 18: Hypervolume (qNParEGO, qUCB) = (3.79, 3.67), time = 3.47.\n",
      "Batch 19: Hypervolume (qNParEGO, qUCB) = (3.79, 3.73), time = 3.22.\n",
      "Batch 20: Hypervolume (qNParEGO, qUCB) = (3.79, 3.75), time = 3.49.\n",
      "Batch 21: Hypervolume (qNParEGO, qUCB) = (3.79, 3.78), time = 3.97.\n",
      "Batch 22: Hypervolume (qNParEGO, qUCB) = (3.80, 3.80), time = 5.60.\n",
      "Batch 23: Hypervolume (qNParEGO, qUCB) = (3.80, 4.07), time = 3.94.\n",
      "Batch 24: Hypervolume (qNParEGO, qUCB) = (3.84, 4.12), time = 5.54.\n",
      "Batch 25: Hypervolume (qNParEGO, qUCB) = (3.84, 4.12), time = 4.15.\n",
      "Batch 26: Hypervolume (qNParEGO, qUCB) = (3.84, 4.14), time = 3.94.\n",
      "Batch 27: Hypervolume (qNParEGO, qUCB) = (3.85, 4.20), time = 4.60.\n",
      "Batch 28: Hypervolume (qNParEGO, qUCB) = (3.85, 4.20), time = 5.37.\n",
      "Batch 29: Hypervolume (qNParEGO, qUCB) = (3.87, 4.21), time = 4.02.\n",
      "Batch 30: Hypervolume (qNParEGO, qUCB) = (3.87, 4.32), time = 4.32.\n",
      "Batch 31: Hypervolume (qNParEGO, qUCB) = (3.87, 4.36), time = 6.03.\n",
      "Batch 32: Hypervolume (qNParEGO, qUCB) = (3.97, 4.36), time = 4.52.\n",
      "Batch 33: Hypervolume (qNParEGO, qUCB) = (3.97, 4.37), time = 6.45.\n",
      "Batch 34: Hypervolume (qNParEGO, qUCB) = (4.02, 4.37), time = 6.50.\n",
      "Batch 35: Hypervolume (qNParEGO, qUCB) = (4.02, 4.37), time = 5.85.\n",
      "Batch 36: Hypervolume (qNParEGO, qUCB) = (4.32, 4.40), time = 5.12.\n",
      "Batch 37: Hypervolume (qNParEGO, qUCB) = (4.32, 4.40), time = 6.58.\n",
      "Batch 38: Hypervolume (qNParEGO, qUCB) = (4.32, 4.40), time = 5.16.\n",
      "Batch 39: Hypervolume (qNParEGO, qUCB) = (4.34, 4.43), time = 6.21.\n",
      "Batch 40: Hypervolume (qNParEGO, qUCB) = (4.43, 4.43), time = 6.59.\n",
      "Batch 41: Hypervolume (qNParEGO, qUCB) = (4.43, 4.47), time = 5.74.\n",
      "Batch 42: Hypervolume (qNParEGO, qUCB) = (4.43, 4.68), time = 6.53.\n",
      "Batch 43: Hypervolume (qNParEGO, qUCB) = (4.48, 4.71), time = 6.28.\n",
      "Batch 44: Hypervolume (qNParEGO, qUCB) = (4.57, 4.71), time = 5.02.\n",
      "Batch 45: Hypervolume (qNParEGO, qUCB) = (4.58, 4.71), time = 6.57.\n",
      "Batch 46: Hypervolume (qNParEGO, qUCB) = (4.59, 4.78), time = 5.04.\n",
      "Batch 47: Hypervolume (qNParEGO, qUCB) = (4.61, 4.99), time = 7.00.\n",
      "Batch 48: Hypervolume (qNParEGO, qUCB) = (4.61, 5.16), time = 7.11.\n",
      "Batch 49: Hypervolume (qNParEGO, qUCB) = (4.64, 5.26), time = 5.48.\n",
      "Batch 50: Hypervolume (qNParEGO, qUCB) = (4.64, 5.26), time = 6.10.\n",
      "Batch 51: Hypervolume (qNParEGO, qUCB) = (4.65, 5.26), time = 7.50.\n",
      "Batch 52: Hypervolume (qNParEGO, qUCB) = (4.78, 5.30), time = 7.88.\n",
      "Batch 53: Hypervolume (qNParEGO, qUCB) = (4.78, 5.32), time = 6.75.\n",
      "Batch 54: Hypervolume (qNParEGO, qUCB) = (4.81, 5.36), time = 6.76.\n",
      "Batch 55: Hypervolume (qNParEGO, qUCB) = (4.87, 5.39), time = 8.15.\n",
      "Batch 56: Hypervolume (qNParEGO, qUCB) = (4.87, 5.44), time = 7.30.\n",
      "Batch 57: Hypervolume (qNParEGO, qUCB) = (4.87, 5.44), time = 8.85.\n",
      "Batch 58: Hypervolume (qNParEGO, qUCB) = (5.15, 5.46), time = 9.41.\n",
      "Batch 59: Hypervolume (qNParEGO, qUCB) = (5.15, 5.61), time = 6.12.\n",
      "Batch 60: Hypervolume (qNParEGO, qUCB) = (5.17, 5.62), time = 7.47.\n",
      "Batch 61: Hypervolume (qNParEGO, qUCB) = (5.17, 5.63), time = 8.40.\n",
      "Batch 62: Hypervolume (qNParEGO, qUCB) = (5.17, 5.63), time = 9.72.\n",
      "Batch 63: Hypervolume (qNParEGO, qUCB) = (5.17, 5.64), time = 9.29.\n",
      "Batch 64: Hypervolume (qNParEGO, qUCB) = (5.28, 5.68), time = 8.26.\n",
      "Batch 65: Hypervolume (qNParEGO, qUCB) = (5.34, 5.83), time = 10.85.\n",
      "Batch 66: Hypervolume (qNParEGO, qUCB) = (5.34, 5.83), time = 6.11.\n",
      "Batch 67: Hypervolume (qNParEGO, qUCB) = (5.40, 5.83), time = 6.54.\n",
      "Batch 68: Hypervolume (qNParEGO, qUCB) = (5.55, 5.83), time = 8.98.\n",
      "Batch 69: Hypervolume (qNParEGO, qUCB) = (5.55, 6.13), time = 7.48.\n",
      "Batch 70: Hypervolume (qNParEGO, qUCB) = (5.69, 6.13), time = 7.98.\n",
      "Batch 71: Hypervolume (qNParEGO, qUCB) = (5.83, 6.23), time = 7.41.\n",
      "Batch 72: Hypervolume (qNParEGO, qUCB) = (5.83, 6.26), time = 12.68.\n",
      "Batch 73: Hypervolume (qNParEGO, qUCB) = (5.83, 6.38), time = 7.36.\n",
      "Batch 74: Hypervolume (qNParEGO, qUCB) = (5.85, 6.42), time = 9.05.\n",
      "Batch 75: Hypervolume (qNParEGO, qUCB) = (5.85, 6.42), time = 8.73.\n",
      "Batch 76: Hypervolume (qNParEGO, qUCB) = (5.87, 6.42), time = 6.37.\n",
      "Batch 77: Hypervolume (qNParEGO, qUCB) = (6.13, 6.42), time = 9.97.\n",
      "Batch 78: Hypervolume (qNParEGO, qUCB) = (6.13, 6.42), time = 10.82.\n",
      "Batch 79: Hypervolume (qNParEGO, qUCB) = (6.13, 6.72), time = 9.56.\n",
      "Batch 80: Hypervolume (qNParEGO, qUCB) = (6.13, 6.72), time = 7.38.\n",
      "Batch 81: Hypervolume (qNParEGO, qUCB) = (6.13, 6.72), time = 11.00.\n",
      "Batch 82: Hypervolume (qNParEGO, qUCB) = (6.18, 6.78), time = 4.67.\n",
      "Batch 83: Hypervolume (qNParEGO, qUCB) = (6.18, 6.78), time = 9.49.\n",
      "Batch 84: Hypervolume (qNParEGO, qUCB) = (6.18, 6.80), time = 6.89.\n",
      "Batch 85: Hypervolume (qNParEGO, qUCB) = (6.18, 6.86), time = 8.05.\n",
      "Batch 86: Hypervolume (qNParEGO, qUCB) = (6.23, 6.86), time = 4.73.\n",
      "Batch 87: Hypervolume (qNParEGO, qUCB) = (6.23, 6.87), time = 4.99.\n",
      "Batch 88: Hypervolume (qNParEGO, qUCB) = (6.23, 6.92), time = 7.14.\n",
      "Batch 89: Hypervolume (qNParEGO, qUCB) = (6.25, 6.96), time = 9.41.\n",
      "Batch 90: Hypervolume (qNParEGO, qUCB) = (6.46, 6.96), time = 16.90.\n",
      "Batch 91: Hypervolume (qNParEGO, qUCB) = (6.64, 6.96), time = 9.01.\n",
      "Batch 92: Hypervolume (qNParEGO, qUCB) = (6.72, 7.13), time = 7.73.\n",
      "Batch 93: Hypervolume (qNParEGO, qUCB) = (6.72, 7.20), time = 9.87.\n",
      "Batch 94: Hypervolume (qNParEGO, qUCB) = (6.97, 7.22), time = 10.92.\n",
      "Batch 95: Hypervolume (qNParEGO, qUCB) = (6.99, 7.23), time = 11.76.\n",
      "Batch 96: Hypervolume (qNParEGO, qUCB) = (6.99, 7.28), time = 5.96.\n",
      "Batch 97: Hypervolume (qNParEGO, qUCB) = (6.99, 7.31), time = 4.35.\n",
      "Batch 98: Hypervolume (qNParEGO, qUCB) = (6.99, 7.31), time = 10.36.\n",
      "Batch 99: Hypervolume (qNParEGO, qUCB) = (6.99, 7.32), time = 5.21.\n",
      "Batch 100: Hypervolume (qNParEGO, qUCB) = (7.00, 7.32), time = 10.54.pref_point tensor([0., 0.])\n",
      "==========\n",
      "tensor([[1.6144, 0.4367],\n",
      "        [1.2252, 1.5506],\n",
      "        [0.4166, 1.3536],\n",
      "        [1.2817, 1.1527],\n",
      "        [1.4254, 0.6449]])\n",
      "==========\n",
      "\n",
      "tensor([[1.6144, 0.4367],\n",
      "        [1.2252, 1.5506],\n",
      "        [0.4166, 1.3536],\n",
      "        [1.2817, 1.1527],\n",
      "        [1.4254, 0.6449]])\n"
     ]
    }
   ],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "N_BATCH = 100 if not SMOKE_TEST else 10\n",
    "MC_SAMPLES = 64 if not SMOKE_TEST else 16\n",
    "\n",
    "verbose = True\n",
    "\n",
    "hvs_qparego, hvs_qucb, = [], []\n",
    "\n",
    "n = 109\n",
    "\n",
    "# call helper functions to generate initial training data and initialize model\n",
    "train_x_qparego, train_obj_qparego, train_obj_true_qparego = generate_initial_data(n=109)\n",
    "mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "\n",
    "train_x_qucb, train_obj_qucb, train_obj_true_qucb = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "\n",
    "mll_qucb, model_qucb = initialize_model(train_x_qucb, train_obj_qucb)\n",
    "\n",
    "# compute hypervolume\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "\n",
    "hvs_qparego.append(volume)\n",
    "hvs_qucb.append(volume)\n",
    "\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(1, N_BATCH + 1):    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # fit the models\n",
    "    fit_gpytorch_model(mll_qparego)\n",
    "    fit_gpytorch_model(mll_qucb)\n",
    "\n",
    "    \n",
    "    # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "    qparego_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "    qucb_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "\n",
    "    \n",
    "    # optimize acquisition functions and get new observations\n",
    "    new_x_qparego, new_obj_qparego, new_obj_true_qparego = optimize_qnparego_and_get_observation(\n",
    "        model_qparego, train_x_qparego, train_obj_qparego, qparego_sampler\n",
    "    )\n",
    "    new_x_qucb, new_obj_qucb, new_obj_true_qucb = optimize_qucb_and_get_observation(\n",
    "        model_qucb, train_x_qucb, train_obj_qucb, qucb_sampler\n",
    "    )\n",
    "            \n",
    "    # update training points\n",
    "    train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "    train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "    train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj_true_qparego])\n",
    "\n",
    "    train_x_qucb = torch.cat([train_x_qucb, new_x_qucb])\n",
    "    train_obj_qucb = torch.cat([train_obj_qucb, new_obj_qucb])\n",
    "    train_obj_true_qucb = torch.cat([train_obj_true_qucb, new_obj_true_qucb])\n",
    "    \n",
    "\n",
    "    # update progress\n",
    "    for hvs_list, train_obj in zip(\n",
    "        (hvs_qparego, hvs_qucb), \n",
    "        ( train_obj_true_qparego, train_obj_true_qucb),\n",
    "    ):\n",
    "        # compute hypervolume\n",
    "        bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj)\n",
    "        volume = bd.compute_hypervolume().item()\n",
    "        hvs_list.append(volume)\n",
    "\n",
    "    # reinitialize the models so they are ready for fitting on next iteration\n",
    "    # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "    # using the hyperparameters from the previous iteration\n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "    mll_qucb, model_qucb = initialize_model(train_x_qucb, train_obj_qucb)\n",
    "\n",
    "    #print(train_x_qucb[0,0])\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nBatch {iteration:>2}: Hypervolume (qNParEGO, qUCB) = \"\n",
    "            f\"({hvs_qparego[-1]:>4.2f}, {hvs_qucb[-1]:>4.2f}), \"\n",
    "            f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
    "        )\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "\n",
    "print(\"pref_point\", problem.ref_point)\n",
    "print(\"==========\")\n",
    "print(train_obj_true_qucb[:5])\n",
    "print(\"==========\\n\")\n",
    "print(train_obj_true_qparego[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f13d4b",
   "metadata": {},
   "source": [
    "### 5 objective DTLZ5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a867c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = DTLZ2(dim=10, num_objectives=5)\n",
    "problem.ref_point = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8757169a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  1: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 6.90.\n",
      "Batch  2: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 6.21.\n",
      "Batch  3: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 5.59.\n",
      "Batch  4: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 5.66.\n",
      "Batch  5: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 6.27.\n",
      "Batch  6: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 6.45.\n",
      "Batch  7: Hypervolume (qNParEGO, qUCB) = (0.48, 0.47), time = 6.98.\n",
      "Batch  8: Hypervolume (qNParEGO, qUCB) = (0.48, 0.48), time = 6.45.\n",
      "Batch  9: Hypervolume (qNParEGO, qUCB) = (0.48, 0.48), time = 8.98.\n",
      "Batch 10: Hypervolume (qNParEGO, qUCB) = (0.49, 0.49), time = 8.61.\n",
      "Batch 11: Hypervolume (qNParEGO, qUCB) = (0.49, 0.50), time = 8.14.\n",
      "Batch 12: Hypervolume (qNParEGO, qUCB) = (0.49, 0.50), time = 6.00.\n",
      "Batch 13: Hypervolume (qNParEGO, qUCB) = (0.49, 0.52), time = 9.82.\n",
      "Batch 14: Hypervolume (qNParEGO, qUCB) = (0.49, 0.52), time = 7.72.\n",
      "Batch 15: Hypervolume (qNParEGO, qUCB) = (0.49, 0.52), time = 8.02.\n",
      "Batch 16: Hypervolume (qNParEGO, qUCB) = (0.49, 0.53), time = 7.31.\n",
      "Batch 17: Hypervolume (qNParEGO, qUCB) = (0.49, 0.53), time = 10.76.\n",
      "Batch 18: Hypervolume (qNParEGO, qUCB) = (0.49, 0.53), time = 7.84.\n",
      "Batch 19: Hypervolume (qNParEGO, qUCB) = (0.49, 0.54), time = 9.74.\n",
      "Batch 20: Hypervolume (qNParEGO, qUCB) = (0.49, 0.54), time = 8.04.\n",
      "Batch 21: Hypervolume (qNParEGO, qUCB) = (0.49, 0.54), time = 6.12.\n",
      "Batch 22: Hypervolume (qNParEGO, qUCB) = (0.49, 0.54), time = 11.65.\n",
      "Batch 23: Hypervolume (qNParEGO, qUCB) = (0.49, 0.55), time = 12.13.\n",
      "Batch 24: Hypervolume (qNParEGO, qUCB) = (0.51, 0.55), time = 7.06.\n",
      "Batch 25: Hypervolume (qNParEGO, qUCB) = (0.51, 0.57), time = 11.95.\n",
      "Batch 26: Hypervolume (qNParEGO, qUCB) = (0.52, 0.57), time = 10.23.\n",
      "Batch 27: Hypervolume (qNParEGO, qUCB) = (0.52, 0.57), time = 9.00.\n",
      "Batch 28: Hypervolume (qNParEGO, qUCB) = (0.52, 0.59), time = 12.66.\n",
      "Batch 29: Hypervolume (qNParEGO, qUCB) = (0.52, 0.59), time = 14.07.\n",
      "Batch 30: Hypervolume (qNParEGO, qUCB) = (0.52, 0.62), time = 13.18.\n",
      "Batch 31: Hypervolume (qNParEGO, qUCB) = (0.52, 0.62), time = 12.46.\n",
      "Batch 32: Hypervolume (qNParEGO, qUCB) = (0.52, 0.62), time = 12.85.\n",
      "Batch 33: Hypervolume (qNParEGO, qUCB) = (0.52, 0.65), time = 17.04.\n",
      "Batch 34: Hypervolume (qNParEGO, qUCB) = (0.52, 0.66), time = 16.11.\n",
      "Batch 35: Hypervolume (qNParEGO, qUCB) = (0.52, 0.75), time = 13.89.\n",
      "Batch 36: Hypervolume (qNParEGO, qUCB) = (0.52, 0.87), time = 16.68.\n",
      "Batch 37: Hypervolume (qNParEGO, qUCB) = (0.52, 0.91), time = 12.44.\n",
      "Batch 38: Hypervolume (qNParEGO, qUCB) = (0.52, 0.95), time = 21.98.\n",
      "Batch 39: Hypervolume (qNParEGO, qUCB) = (0.52, 0.95), time = 18.17.\n",
      "Batch 40: Hypervolume (qNParEGO, qUCB) = (0.52, 0.95), time = 22.49.\n",
      "Batch 41: Hypervolume (qNParEGO, qUCB) = (0.52, 0.97), time = 16.23.\n",
      "Batch 42: Hypervolume (qNParEGO, qUCB) = (0.52, 0.97), time = 23.03.\n",
      "Batch 43: Hypervolume (qNParEGO, qUCB) = (0.52, 0.97), time = 27.47.\n",
      "Batch 44: Hypervolume (qNParEGO, qUCB) = (0.52, 1.00), time = 22.69.\n",
      "Batch 45: Hypervolume (qNParEGO, qUCB) = (0.52, 1.00), time = 23.44.\n",
      "Batch 46: Hypervolume (qNParEGO, qUCB) = (0.53, 1.01), time = 16.69.\n",
      "Batch 47: Hypervolume (qNParEGO, qUCB) = (0.53, 1.01), time = 27.93.\n",
      "Batch 48: Hypervolume (qNParEGO, qUCB) = (0.53, 1.01), time = 29.66.\n",
      "Batch 49: Hypervolume (qNParEGO, qUCB) = (0.53, 1.01), time = 16.47.\n",
      "Batch 50: Hypervolume (qNParEGO, qUCB) = (0.53, 1.03), time = 26.98.\n",
      "Batch 51: Hypervolume (qNParEGO, qUCB) = (0.53, 1.03), time = 27.63.\n",
      "Batch 52: Hypervolume (qNParEGO, qUCB) = (0.53, 1.04), time = 22.73.\n",
      "Batch 53: Hypervolume (qNParEGO, qUCB) = (0.53, 1.04), time = 39.35.\n",
      "Batch 54: Hypervolume (qNParEGO, qUCB) = (0.53, 1.04), time = 22.95.\n",
      "Batch 55: Hypervolume (qNParEGO, qUCB) = (0.53, 1.05), time = 30.70.\n",
      "Batch 56: Hypervolume (qNParEGO, qUCB) = (0.53, 1.05), time = 17.75.\n",
      "Batch 57: Hypervolume (qNParEGO, qUCB) = (0.53, 1.06), time = 19.71.\n",
      "Batch 58: Hypervolume (qNParEGO, qUCB) = (0.53, 1.06), time = 16.73.\n",
      "Batch 59: Hypervolume (qNParEGO, qUCB) = (0.55, 1.06), time = 34.48.\n",
      "Batch 60: Hypervolume (qNParEGO, qUCB) = (0.55, 1.07), time = 36.50.\n",
      "Batch 61: Hypervolume (qNParEGO, qUCB) = (0.55, 1.07), time = 27.23.\n",
      "Batch 62: Hypervolume (qNParEGO, qUCB) = (0.55, 1.07), time = 24.95.\n",
      "Batch 63: Hypervolume (qNParEGO, qUCB) = (0.56, 1.07), time = 15.49.\n",
      "Batch 64: Hypervolume (qNParEGO, qUCB) = (0.56, 1.07), time = 23.67.\n",
      "Batch 65: Hypervolume (qNParEGO, qUCB) = (0.56, 1.07), time = 30.61.\n",
      "Batch 66: Hypervolume (qNParEGO, qUCB) = (0.56, 1.07), time = 25.88.\n",
      "Batch 67: Hypervolume (qNParEGO, qUCB) = (0.56, 1.08), time = 31.90.\n",
      "Batch 68: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 44.63.\n",
      "Batch 69: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 23.78.\n",
      "Batch 70: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 33.64.\n",
      "Batch 71: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 40.22.\n",
      "Batch 72: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 26.83.\n",
      "Batch 73: Hypervolume (qNParEGO, qUCB) = (0.56, 1.09), time = 27.65.\n",
      "Batch 74: Hypervolume (qNParEGO, qUCB) = (0.57, 1.10), time = 35.58.\n",
      "Batch 75: Hypervolume (qNParEGO, qUCB) = (0.57, 1.10), time = 29.41.\n",
      "Batch 76: Hypervolume (qNParEGO, qUCB) = (0.57, 1.10), time = 41.73.\n",
      "Batch 77: Hypervolume (qNParEGO, qUCB) = (0.57, 1.10), time = 28.33.\n",
      "Batch 78: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 36.17.\n",
      "Batch 79: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 36.82.\n",
      "Batch 80: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 35.78.\n",
      "Batch 81: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 41.48.\n",
      "Batch 82: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 24.14.\n",
      "Batch 83: Hypervolume (qNParEGO, qUCB) = (0.57, 1.11), time = 41.40.\n",
      "Batch 84: Hypervolume (qNParEGO, qUCB) = (0.58, 1.11), time = 41.29.\n",
      "Batch 85: Hypervolume (qNParEGO, qUCB) = (0.58, 1.11), time = 40.27.\n",
      "Batch 86: Hypervolume (qNParEGO, qUCB) = (0.58, 1.11), time = 24.36.\n",
      "Batch 87: Hypervolume (qNParEGO, qUCB) = (0.59, 1.11), time = 30.90.\n",
      "Batch 88: Hypervolume (qNParEGO, qUCB) = (0.59, 1.12), time = 22.26.\n",
      "Batch 89: Hypervolume (qNParEGO, qUCB) = (0.59, 1.12), time = 34.37.\n",
      "Batch 90: Hypervolume (qNParEGO, qUCB) = (0.59, 1.12), time = 43.76.\n",
      "Batch 91: Hypervolume (qNParEGO, qUCB) = (0.60, 1.13), time = 45.52.\n",
      "Batch 92: Hypervolume (qNParEGO, qUCB) = (0.60, 1.13), time = 40.73.\n",
      "Batch 93: Hypervolume (qNParEGO, qUCB) = (0.60, 1.13), time = 50.14.\n",
      "Batch 94: Hypervolume (qNParEGO, qUCB) = (0.60, 1.14), time = 50.00.\n",
      "Batch 95: Hypervolume (qNParEGO, qUCB) = (0.60, 1.16), time = 42.22.\n",
      "Batch 96: Hypervolume (qNParEGO, qUCB) = (0.60, 1.19), time = 58.44.\n",
      "Batch 97: Hypervolume (qNParEGO, qUCB) = (0.60, 1.19), time = 40.92.\n",
      "Batch 98: Hypervolume (qNParEGO, qUCB) = (0.60, 1.21), time = 55.55.\n",
      "Batch 99: Hypervolume (qNParEGO, qUCB) = (0.60, 1.22), time = 38.66.\n",
      "Batch 100: Hypervolume (qNParEGO, qUCB) = (0.61, 1.22), time = 28.88.pref_point tensor([0., 0., 0., 0., 0.])\n",
      "==========\n",
      "tensor([[0.3654, 0.2230, 0.1732, 1.8618, 0.3581],\n",
      "        [0.0386, 0.0942, 0.1504, 0.1523, 1.7086],\n",
      "        [0.2803, 0.0908, 0.2461, 0.8895, 1.0399],\n",
      "        [0.1495, 0.2594, 1.0042, 0.1309, 0.8927],\n",
      "        [0.2462, 0.3384, 0.7321, 0.8517, 0.6480]])\n",
      "==========\n",
      "\n",
      "tensor([[0.3654, 0.2230, 0.1732, 1.8618, 0.3581],\n",
      "        [0.0386, 0.0942, 0.1504, 0.1523, 1.7086],\n",
      "        [0.2803, 0.0908, 0.2461, 0.8895, 1.0399],\n",
      "        [0.1495, 0.2594, 1.0042, 0.1309, 0.8927],\n",
      "        [0.2462, 0.3384, 0.7321, 0.8517, 0.6480]])\n"
     ]
    }
   ],
   "source": [
    "N_BATCH = 100 if not SMOKE_TEST else 10\n",
    "MC_SAMPLES = 64 if not SMOKE_TEST else 16\n",
    "\n",
    "verbose = True\n",
    "\n",
    "hvs_qparego, hvs_qucb, = [], []\n",
    "\n",
    "n = 109\n",
    "\n",
    "# call helper functions to generate initial training data and initialize model\n",
    "train_x_qparego, train_obj_qparego, train_obj_true_qparego = generate_initial_data(n=109)\n",
    "mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "\n",
    "train_x_qucb, train_obj_qucb, train_obj_true_qucb = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "\n",
    "mll_qucb, model_qucb = initialize_model(train_x_qucb, train_obj_qucb)\n",
    "\n",
    "# compute hypervolume\n",
    "bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "\n",
    "hvs_qparego.append(volume)\n",
    "hvs_qucb.append(volume)\n",
    "\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(1, N_BATCH + 1):    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # fit the models\n",
    "    fit_gpytorch_model(mll_qparego)\n",
    "    fit_gpytorch_model(mll_qucb)\n",
    "\n",
    "    \n",
    "    # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "    qparego_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "    qucb_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "\n",
    "    \n",
    "    # optimize acquisition functions and get new observations\n",
    "    new_x_qparego, new_obj_qparego, new_obj_true_qparego = optimize_qnparego_and_get_observation(\n",
    "        model_qparego, train_x_qparego, train_obj_qparego, qparego_sampler\n",
    "    )\n",
    "    new_x_qucb, new_obj_qucb, new_obj_true_qucb = optimize_qucb_and_get_observation(\n",
    "        model_qucb, train_x_qucb, train_obj_qucb, qucb_sampler\n",
    "    )\n",
    "            \n",
    "    # update training points\n",
    "    train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "    train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "    train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj_true_qparego])\n",
    "\n",
    "    train_x_qucb = torch.cat([train_x_qucb, new_x_qucb])\n",
    "    train_obj_qucb = torch.cat([train_obj_qucb, new_obj_qucb])\n",
    "    train_obj_true_qucb = torch.cat([train_obj_true_qucb, new_obj_true_qucb])\n",
    "    \n",
    "\n",
    "    # update progress\n",
    "    for hvs_list, train_obj in zip(\n",
    "        (hvs_qparego, hvs_qucb), \n",
    "        ( train_obj_true_qparego, train_obj_true_qucb),\n",
    "    ):\n",
    "        # compute hypervolume\n",
    "        bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj)\n",
    "        volume = bd.compute_hypervolume().item()\n",
    "        hvs_list.append(volume)\n",
    "\n",
    "    # reinitialize the models so they are ready for fitting on next iteration\n",
    "    # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "    # using the hyperparameters from the previous iteration\n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "    mll_qucb, model_qucb = initialize_model(train_x_qucb, train_obj_qucb)\n",
    "\n",
    "    #print(train_x_qucb[0,0])\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nBatch {iteration:>2}: Hypervolume (qNParEGO, qUCB) = \"\n",
    "            f\"({hvs_qparego[-1]:>4.2f}, {hvs_qucb[-1]:>4.2f}), \"\n",
    "            f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
    "        )\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "\n",
    "print(\"pref_point\", problem.ref_point)\n",
    "print(\"==========\")\n",
    "print(train_obj_true_qucb[:5])\n",
    "print(\"==========\\n\")\n",
    "print(train_obj_true_qparego[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab9c7f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Reference points for both ParEGO and UCB are [0.,0.] for 2 objective case and [0., 0., 0., 0., 0.] for 5 objective case.\n",
    "\n",
    "\n",
    "Note that Botorch assumes maximization so thats why using UCB and reference point for HV at [0,0] works. Botorch uses parallel batch versions of the algorithms and calls each iteration the real objective function only once. Code will take long to run with 100 iterations, my pc gets about 9 seconds per iteration for 2 objective case and nearing to 30 seconds per iteration for the 5 objective case (with RTX 3070). Also iteration number goes up, generally each batch takes a bit longer to compute especially with the 5 objective case.\n",
    "The given 109 starting points would suggest that bigger batch size than 4 could be used, but since that increases the time each iteration takes, I stuck with batch size of 4 even though possibly better results would be gotten using bigger batch size.\n",
    "\n",
    "\n",
    "|  |    HV (qnParEGO) |  HV (UCB) |\n",
    "| --- | --- | --- |\n",
    "| DTLZ2 (K=2, n=10) | 7.00   | 7.32  \n",
    "| DTLZ2 (K=5, n=10) | 0.61   | 1.22  \n",
    "\n",
    "\n",
    "UCB seems to work better in both cases even though ParEGO is usually tiny bit better in the first batch or two.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38datademos_env",
   "language": "python",
   "name": "38datademos_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
